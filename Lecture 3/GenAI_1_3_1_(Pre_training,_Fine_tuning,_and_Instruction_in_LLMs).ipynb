{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijY9RF1h_kp"
      },
      "source": [
        "### **Understanding Pre-training, Fine-tuning, and Instruction-tuning in Large Language Models (LLMs)**\n",
        "\n",
        "In the journey of building and optimizing Large Language Models (LLMs), three critical stages determine how well the model can understand, generalize, and adapt to different tasks:\n",
        "\n",
        "1. **Pre-training**  \n",
        "2. **Fine-tuning**  \n",
        "3. **Instruction-tuning**  \n",
        "\n",
        "Let's dive deeper into what each of these stages entails.\n",
        "\n",
        "---\n",
        "\n",
        "## üî• **1. Pre-training**  \n",
        "**Pre-training** is the initial phase in which an LLM is trained on massive datasets consisting of diverse, unlabeled text from books, articles, websites, and more. The goal is to enable the model to learn the general structure of language, grammar, syntax, and world knowledge.\n",
        "\n",
        "### ‚úÖ **Key Characteristics:**\n",
        "- **Objective:** Learn general language patterns and representations.\n",
        "- **Data:** Large, diverse, unlabeled text datasets.\n",
        "- **Method:** Self-supervised learning, where the model predicts missing tokens (masked language modeling) or the next word (causal language modeling).\n",
        "- **Time & Resources:** Requires massive computational resources and long training times.\n",
        "\n",
        "### üìö **Example:**\n",
        "- **GPT-3** was pre-trained on a massive corpus of internet text to develop an understanding of human language.\n",
        "- **BERT** was pre-trained using a masked language modeling (MLM) objective.\n",
        "\n",
        "### üß† **How It Works:**\n",
        "- The model learns how words and phrases typically occur together.\n",
        "- It becomes proficient in grammar, sentence structure, and general world facts.\n",
        "- However, it doesn't specialize in specific tasks (like sentiment analysis or summarization).\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **2. Fine-tuning**  \n",
        "**Fine-tuning** involves taking a pre-trained model and training it further on a **smaller, task-specific dataset**. This process adapts the model to perform better on a specific task, such as text classification, question answering, or translation.\n",
        "\n",
        "### ‚úÖ **Key Characteristics:**\n",
        "- **Objective:** Specialize the model for a particular task.\n",
        "- **Data:** Labeled, task-specific datasets (e.g., movie reviews for sentiment analysis).\n",
        "- **Method:** Supervised learning, where the model learns from labeled examples.\n",
        "- **Time & Resources:** Less intensive than pre-training but requires careful tuning to avoid overfitting.\n",
        "\n",
        "### üìö **Example:**\n",
        "- **BERT for Sentiment Analysis:** Fine-tuned on datasets like SST-2 (Stanford Sentiment Treebank) to classify text sentiment.\n",
        "- **RoBERTa for Question-Answering:** Fine-tuned on the SQuAD dataset.\n",
        "\n",
        "### üß† **How It Works:**\n",
        "- The base knowledge from pre-training is leveraged.\n",
        "- Model weights are updated to focus on patterns relevant to the specific task.\n",
        "- Fine-tuning helps the model generalize better for similar types of data.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ **3. Instruction-tuning**  \n",
        "**Instruction-tuning** is an advanced technique where the model is fine-tuned on **datasets that contain human-written instructions** paired with expected outputs. The aim is to make the model better at **following instructions** across various tasks.\n",
        "\n",
        "### ‚úÖ **Key Characteristics:**\n",
        "- **Objective:** Teach the model to follow human instructions effectively.\n",
        "- **Data:** Datasets with prompts/instructions and their corresponding responses.\n",
        "- **Method:** Supervised fine-tuning, but with a focus on multi-task datasets that encourage instruction following.\n",
        "- **Outcome:** A more versatile, instruction-aware model that can handle diverse prompts with minimal additional training.\n",
        "\n",
        "### üìö **Example:**\n",
        "- **InstructGPT:** Fine-tuned to better follow human instructions using feedback from human annotators.\n",
        "- **Llama-2 Chat Models:** Instruction-tuned to perform better in conversational tasks.\n",
        "\n",
        "### üß† **How It Works:**\n",
        "- The model is exposed to a wide variety of tasks through instructional prompts (like \"Summarize this text\" or \"Translate this sentence\").\n",
        "- It learns how to interpret and respond accurately to a broader range of human instructions.\n",
        "- This makes the model **more generalizable and user-friendly** for various tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ **Comparison Table**\n",
        "\n",
        "| **Aspect**        | **Pre-training**                                         | **Fine-tuning**                                            | **Instruction-tuning**                                       |\n",
        "|--------------------|---------------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------------|\n",
        "| **Purpose**        | Learn general language patterns and world knowledge     | Specialize for a specific task                              | Teach the model to follow diverse human instructions          |\n",
        "| **Data**           | Large, unlabeled, diverse datasets                      | Smaller, labeled task-specific datasets                     | Instruction-response datasets (e.g., human prompts and outputs) |\n",
        "| **Learning Type**  | Self-supervised                                         | Supervised                                                  | Supervised with a focus on task instructions                   |\n",
        "| **Outcome**        | General understanding of language                       | Specialized performance for a task                          | Ability to generalize and follow diverse instructions          |\n",
        "| **Example**        | GPT-3's general pre-training                            | Fine-tuning BERT for sentiment classification               | Fine-tuning GPT with instruction-following tasks               |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **When to Use Each Approach?**\n",
        "\n",
        "1. **Pre-training:**  \n",
        "   - When creating a brand-new language model from scratch.  \n",
        "   - Requires vast resources, but gives a foundational model with broad capabilities.\n",
        "\n",
        "2. **Fine-tuning:**  \n",
        "   - When you want to specialize a pre-trained model for a **specific task**.  \n",
        "   - Ideal for use cases like spam detection, sentiment analysis, or document classification.\n",
        "\n",
        "3. **Instruction-tuning:**  \n",
        "   - When building a model that can **handle multiple tasks** by following clear instructions.  \n",
        "   - Best for chatbots, AI assistants, or general-purpose LLMs that need to understand diverse prompts.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Real-World Example:**\n",
        "\n",
        "1. **Pre-training Phase:**  \n",
        "   OpenAI pre-trained **GPT-3** on a massive internet corpus to give it broad language understanding.\n",
        "\n",
        "2. **Fine-tuning Phase:**  \n",
        "   Specific versions of GPT-3 were fine-tuned for tasks like coding, medical data analysis, or summarization.\n",
        "\n",
        "3. **Instruction-tuning Phase:**  \n",
        "   **InstructGPT** was fine-tuned to understand and follow instructions better, improving its interaction quality in real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Why is Instruction-tuning Important Today?**\n",
        "- Modern LLMs are used in diverse real-world applications where they need to follow user instructions accurately.  \n",
        "- Instruction-tuning makes models safer, more reliable, and easier to interact with.  \n",
        "- It is the reason why models like **ChatGPT** and **Llama-2 Chat** perform so well in interactive conversations.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14FSYqpgjXyP"
      },
      "source": [
        "#Finetuning Without Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1HePrmWj-IG"
      },
      "source": [
        "**[AutoTrain Advance](https://huggingface.co/docs/autotrain/v0.8.24/tasks/llm_finetuning)**\n",
        "\n",
        "**With AutoTrain, you can easily finetune large language models (LLMs) on your own data. You can use AutoTrain to finetune LLMs for a variety of tasks, such as text generation, text classification, and text summarization. You can also use AutoTrain to finetune LLMs for specific use cases, such as chatbots, question-answering systems, and code generation and even basic fine-tuning tasks like classic text generation.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvgOkduYkYa8"
      },
      "source": [
        "# Finetuning With low Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3ZVrj8ojcTR",
        "outputId": "35692e13-2ad9-48ac-b3cb-488a1c76f5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-12 17:32:41\u001b[0m | \u001b[36mautotrain.cli.run_app\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mAutoTrain Public URL: NgrokTunnel: \"https://bfeb-34-168-242-251.ngrok-free.app\" -> \"http://localhost:7860\"\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-12 17:32:41\u001b[0m | \u001b[36mautotrain.cli.run_app\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mPlease wait for the app to load...\u001b[0m\n",
            "INFO     | 2025-03-12 17:32:47 | autotrain.app.ui_routes:<module>:31 - Starting AutoTrain...\n",
            "INFO     | 2025-03-12 17:32:52 | autotrain.app.ui_routes:<module>:315 - AutoTrain started successfully\n",
            "INFO     | 2025-03-12 17:32:52 | autotrain.app.app:<module>:13 - Starting AutoTrain...\n",
            "INFO     | 2025-03-12 17:32:52 | autotrain.app.app:<module>:23 - AutoTrain version: 0.8.36\n",
            "INFO     | 2025-03-12 17:32:52 | autotrain.app.app:<module>:24 - AutoTrain started successfully\n",
            "INFO:     Started server process [1954]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:7860 (Press CTRL+C to quit)\n",
            "INFO:     103.151.172.27:0 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/ HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /static/scripts/fetch_data_and_update_models.js?cb=2025-03-12%2017:33:20 HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /static/scripts/poll.js?cb=2025-03-12%2017:33:20 HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /static/scripts/listeners.js?cb=2025-03-12%2017:33:20 HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /static/scripts/logs.js?cb=2025-03-12%2017:33:20 HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /static/scripts/utils.js?cb=2025-03-12%2017:33:20 HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/model_choices/llm%3Asft HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO     | 2025-03-12 17:33:23 | autotrain.app.ui_routes:fetch_params:415 - Task: llm:sft\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/params/llm%3Asft/basic HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "ERROR    | 2025-03-12 17:33:56 | autotrain.app.utils:token_verification:134 - Failed to request /api/whoami-v2 - ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=3)\"))\n",
            "ERROR    | 2025-03-12 17:33:56 | autotrain.app.ui_routes:user_authentication:343 - Failed to verify token: Hugging Face Hub (/api/whoami-v2) is unreachable, please try again later.\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 401 Unauthorized\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "ERROR    | 2025-03-12 17:34:57 | autotrain.app.utils:token_verification:134 - Failed to request /api/whoami-v2 - ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=3)\"))\n",
            "ERROR    | 2025-03-12 17:34:57 | autotrain.app.ui_routes:user_authentication:343 - Failed to verify token: Hugging Face Hub (/api/whoami-v2) is unreachable, please try again later.\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 401 Unauthorized\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     103.151.172.27:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/is_model_training HTTP/1.1\" 200 OK\n",
            "INFO:     141.11.146.79:0 - \"GET /ui/accelerators HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "#@title ü§ó AutoTrain\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - Enter your [Hugging Face Write Token](https://huggingface.co/settings/tokens)\n",
        "#@markdown - Enter your [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "huggingface_token = '' # @param {type:\"string\"}\n",
        "ngrok_token = \"\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown\n",
        "#@markdown - Attach appropriate accelerator `Runtime > Change runtime type > Hardware accelerator`\n",
        "#@markdown - click `Runtime > Run all`\n",
        "#@markdown - Follow the link to access the UI\n",
        "#@markdown - Training happens inside this Google Colab\n",
        "#@markdown - report issues / feature requests [here](https://github.com/huggingface/autotrain-advanced/issues)\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = str(huggingface_token)\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = str(ngrok_token)\n",
        "os.environ[\"AUTOTRAIN_LOCAL\"] = \"1\"\n",
        "\n",
        "!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "!autotrain app --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMm9IxXJlB2Q"
      },
      "source": [
        "# Another example of finetuning using Autotrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6ZsHZiilK_J"
      },
      "outputs": [],
      "source": [
        "from autotrain.params import LLMTrainingParams\n",
        "from autotrain.project import AutoTrainProject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkD91NR9kz27"
      },
      "outputs": [],
      "source": [
        "HF_USERNAME = \"\"\n",
        "HF_TOKEN = \"\" # get it from https://huggingface.co/settings/token\n",
        "# It is recommended to use secrets or environment variables to store your HF_TOKEN\n",
        "# your token is required if push_to_hub is set to True or if you are accessing a gated model/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMgTQNAdlSPe"
      },
      "outputs": [],
      "source": [
        "params = LLMTrainingParams(\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    data_path=\"HuggingFaceH4/no_robots\", # path to the dataset on huggingface hub\n",
        "    chat_template=\"tokenizer\", # using the chat template defined in the model's tokenizer\n",
        "    text_column=\"messages\", # the column in the dataset that contains the text\n",
        "    train_split=\"train\",\n",
        "    trainer=\"sft\", # using the SFT trainer, choose from sft, default, orpo, dpo and reward\n",
        "    epochs=3,\n",
        "    batch_size=1,\n",
        "    lr=1e-5,\n",
        "    peft=True, # training LoRA using PEFT\n",
        "    quantization=\"int4\", # using int4 quantization\n",
        "    target_modules=\"all-linear\",\n",
        "    padding=\"right\",\n",
        "    optimizer=\"paged_adamw_8bit\",\n",
        "    scheduler=\"cosine\",\n",
        "    gradient_accumulation=8,\n",
        "    mixed_precision=\"bf16\",\n",
        "    merge_adapter=True,\n",
        "    project_name=\"autotrain-llama32-1b-finetune\",\n",
        "    log=\"tensorboard\",\n",
        "    push_to_hub=True,\n",
        "    username=HF_USERNAME,\n",
        "    token=HF_TOKEN,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk8dY9hVlWsX"
      },
      "source": [
        "If your dataset is in CSV / JSONL format (JSONL is most preferred) and is stored locally, make the following changes to `params`:\n",
        "\n",
        "```python\n",
        "params = LLMTrainingParams(\n",
        "    data_path=\"data/\", # this is the path to folder where train.jsonl/train.csv is located\n",
        "    text_column=\"text\", # this is the column name in the CSV/JSONL file which contains the text\n",
        "    train_split = \"train\" # this is the filename without extension\n",
        "    .\n",
        "    .\n",
        "    .\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PZqpLvGlb0P"
      },
      "outputs": [],
      "source": [
        "# this will train the model locally\n",
        "project = AutoTrainProject(params=params, backend=\"local\", process=True)\n",
        "project.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0fvdoWX8Zkd"
      },
      "source": [
        "### **Exercise: Fine-Tuning a Chatbot Model Using AutoTrain on Google Colab (GPU)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Objective**\n",
        "Fine-tuning an open-source language model for a chatbot use case using the Hugging Face AutoTrain library on Google Colab with GPU support. By the end of this exercise, You will understand how to select a model, prepare a dataset, and perform fine-tuning using AutoTrain.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Define the Use Case**\n",
        "**Chatbot Development:** Fine-tune a language model to create a chatbot capable of engaging in natural, multi-turn conversations, answering common queries, and providing assistance across various topics.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Select an Open-Source Model**\n",
        "\n",
        "Choose a model supported by Hugging Face for chat applications:\n",
        "\n",
        "- **`meta-llama/Llama-2-7b-chat-hf`** - Optimized for chat applications.\n",
        "- **`tiiuae/falcon-7b-instruct`** - Lightweight and efficient for conversational AI.\n",
        "- **`mistralai/Mistral-7B-Instruct-v0.1`** - A balanced choice for efficient chat use cases.\n",
        "\n",
        "*For this exercise, we recommend using **`meta-llama/Llama-2-7b-chat-hf`** to leverage its conversational optimization.*\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Select and Prepare the Dataset**\n",
        "\n",
        "- **Dataset Name:** `guanaco-sharegpt-style`  \n",
        "- **Source:** Hugging Face Datasets  \n",
        "- **Description:** Contains multi-turn conversations structured in a ShareGPT format.\n",
        "\n",
        "### **Dataset Preparation Instructions**\n",
        "\n",
        "1. **Load the Dataset**  \n",
        "   Use Hugging Face's `datasets` library to load the `guanaco-sharegpt-style` dataset.\n",
        "\n",
        "2. **Format the Data**  \n",
        "   Each conversation should follow this format:\n",
        "\n",
        "   ```json\n",
        "   [\n",
        "     {\"from\": \"human\", \"value\": \"Hello! How are you?\"},\n",
        "     {\"from\": \"gpt\", \"value\": \"I'm great, thank you! How can I assist you today?\"}\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Set Up Google Colab**\n",
        "\n",
        "1. **Create a New Notebook**  \n",
        "   Open Google Colab and start a new notebook.\n",
        "\n",
        "2. **Enable GPU**  \n",
        "   Navigate to `Runtime > Change runtime type > Hardware accelerator > GPU`.\n",
        "\n",
        "3. **Install Dependencies**  \n",
        "   Install the required libraries:\n",
        "\n",
        "   - `transformers`\n",
        "   - `datasets`\n",
        "   - `autotrain-advanced`\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 5: Fine-Tuning Process with AutoTrain**\n",
        "\n",
        "1. **Initialize AutoTrain**  \n",
        "   Import the `AutoTrain` library and set up the configuration.\n",
        "\n",
        "2. **Configure Training Parameters**  \n",
        "   - **Model Name:** `meta-llama/Llama-2-7b-chat-hf`\n",
        "   - **Task Type:** Text Generation (Chatbot)\n",
        "   - **Number of Epochs:** 3-5\n",
        "   - **Batch Size:** 16 or 32 (depending on GPU capacity)\n",
        "   - **Learning Rate:** Start with `5e-5`\n",
        "   - **Evaluation Strategy:** Evaluate at the end of each epoch.\n",
        "\n",
        "3. **Begin Training**  \n",
        "   Use AutoTrain's API to start the fine-tuning process on the cleaned dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 6: Evaluate the Model**\n",
        "\n",
        "1. **Model Metrics**  \n",
        "   Evaluate the model based on:\n",
        "   - **Perplexity:** To measure how well the model predicts text.\n",
        "   - **Response Coherence:** Check the model's ability to produce human-like responses.\n",
        "\n",
        "2. **Overfitting Check**  \n",
        "   Compare training and validation losses after each epoch.\n",
        "\n",
        "3. **Sample Testing**  \n",
        "   Interact with the fine-tuned chatbot to assess conversational quality.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 7: Save and Export the Model**\n",
        "\n",
        "1. **Save the Model**  \n",
        "   Use AutoTrain‚Äôs `save_pretrained` function to save the model.\n",
        "\n",
        "2. **Push to Hugging Face Hub (Optional)**  \n",
        "   Optionally, upload the model to the Hugging Face Model Hub for public access.\n",
        "\n",
        "3. **Export Locally**  \n",
        "   Download the model for local deployment.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 8: Reflection Questions**\n",
        "\n",
        "- **What were the key challenges faced during dataset preparation and training?**\n",
        "- **How did adjusting the learning rate or batch size impact model performance?**\n",
        "- **What further improvements can be made to enhance the chatbot's responses?**\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
